{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning a distilbert model using only a softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2060 with Max-Q Design True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0), torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from itertools import combinations\n",
    "from sentence_transformers import evaluation, InputExample, losses, models, SentencesDataset, SentenceTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_pairs(lab, strings):\n",
    "    \"\"\"returns pairs from a list of strings\"\"\"\n",
    "    remainder = len(strings) % 2\n",
    "    if remainder != 0:\n",
    "        strings = strings[:-remainder]\n",
    "    num_pairs = int(len(strings)/2)\n",
    "    pairs = []\n",
    "    for i in range(num_pairs):\n",
    "        begin = i*2\n",
    "        end = begin + 2\n",
    "        pair = strings[begin:end]\n",
    "        pairs.append(pair)\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def return_examples(df_):\n",
    "    \"\"\"returns paired reviews from the same class for\n",
    "    training SBERT model\"\"\"\n",
    "    examples = []\n",
    "    for lab in label_dict.values():\n",
    "        lab_df = df_[df_[\"labels\"]==lab]\n",
    "        lab_df = shuffle(lab_df)\n",
    "        texts = list(lab_df[\"text\"].values)\n",
    "        assert len(texts) % 2 == 0\n",
    "        l = int(len(texts)/2)\n",
    "        assert l > 0\n",
    "        labs = [lab] * l\n",
    "        zipped = list(zip(texts[:l], texts[l:], labs))\n",
    "        assert len(zipped) == l \n",
    "        assert l == len(texts)/2\n",
    "        for ex in zipped:\n",
    "            assert type(ex[0]) == type(ex[1]) == str\n",
    "            pair = [ex[0], ex[1]]\n",
    "            assert type(pair) == list\n",
    "            assert type(ex[2]) == int\n",
    "            ex = InputExample(texts=pair, label=ex[2])\n",
    "            examples.append(ex)\n",
    "    return examples\n",
    "\n",
    "\n",
    "def return_contrast_examples(df_, num_pos, num_neg):\n",
    "    \"\"\"single call to get positive and negative examples for\n",
    "    contrastive loss\"\"\"\n",
    "    pos = return_positive_contrast_examples(df_, num_pos)\n",
    "    neg = return_negative_contrast_examples(df_, num_neg)\n",
    "    print(\"Contrastive loss examples total: {}\".format(len(pos + neg)))\n",
    "    return pos + neg\n",
    "\n",
    "\n",
    "def return_positive_contrast_examples(df_, n):\n",
    "    \"\"\"positive examples for contrastive loss:\n",
    "    same class, labeled 1\"\"\"\n",
    "    pos_contrast_examples = []\n",
    "    for lab in label_dict.values():\n",
    "        lab_df = df_[df_[\"labels\"]==lab]\n",
    "        lab_df = shuffle(lab_df)\n",
    "        lab_df = lab_df.sample(n)\n",
    "        revs = [rev.lower() for rev in lab_df[\"text\"].values]\n",
    "        pairs = output_pairs(lab, revs)\n",
    "        for pair in pairs:\n",
    "            assert type(pair) == list\n",
    "            assert len(pair) == 2\n",
    "            assert type(pair[0]) == type(pair[1]) == str\n",
    "            ex = InputExample(texts=pair, label=1)\n",
    "            pos_contrast_examples.append(ex)\n",
    "    return pos_contrast_examples\n",
    "    \n",
    "    \n",
    "def return_negative_contrast_examples(df_, n):\n",
    "    \"\"\"negative examples for contrastive loss:\n",
    "    pairs from all combinations of classes,\n",
    "    with label = 0 indicating they are not\n",
    "    of teh same class\"\"\"\n",
    "    neg_contrast_examples = []\n",
    "    df_dict = {}\n",
    "    for lab in label_dict.values():\n",
    "        lab_df = df_[df_[\"labels\"]==lab]\n",
    "        lab_df = shuffle(lab_df)\n",
    "        lab_df = lab_df.sample(n)\n",
    "        df_dict[lab] = lab_df\n",
    "    for k, comb in enumerate(combinations(df_dict.keys(), 2)):\n",
    "        lab_i, lab_j = comb\n",
    "        df_i = df_dict[lab_i].sample(n)\n",
    "        df_j = df_dict[lab_j].sample(n)\n",
    "        texts_i = list(df_i[\"text\"].values)\n",
    "        texts_j = list(df_j[\"text\"].values)\n",
    "        neg_labs = [0] * len(texts_i)\n",
    "        assert len(neg_labs) == len(texts_i) == len(texts_j)\n",
    "        exes = list(zip(texts_i, texts_j, neg_labs))\n",
    "        for ex in exes:\n",
    "            assert type(ex[0]) == type(ex[1]) == str\n",
    "            assert ex[2] == 0\n",
    "            ex = InputExample(texts=[ex[0].lower(), ex[1].lower()], label=ex[2])\n",
    "            neg_contrast_examples.append(ex)\n",
    "    return neg_contrast_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and specify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392000, 5)\n",
      "(98000, 5)\n",
      "Num. occupations: 49. Training set shape: (392000, 5). Dev set shape: (98000, 5).\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_json(\"train_df_final.json\")\n",
    "dev_df = pd.read_json(\"dev_df_final.json\")\n",
    "\n",
    "print(train_df.shape)\n",
    "print(dev_df.shape)\n",
    "\n",
    "classes = sorted(list(set(train_df[\"occupation\"].values)))\n",
    "label_dict = {}\n",
    "\n",
    "for i, cat in enumerate(classes):\n",
    "    label_dict[cat] = i\n",
    "    \n",
    "print(\"Num. occupations: {}. Training set shape: {}. Dev set shape: {}.\".format(len(label_dict.keys()), \n",
    "                                                                                train_df.shape, dev_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL ARCHITECTURE -- commented out to avoid overwriting model.\n",
    "orig_model_path = os.path.join(os.getcwd(), \"final_proj_model_TUNING_softmax_only/\")\n",
    "if not os.path.isdir(orig_model_path):\n",
    "    os.mkdir(orig_model_path)\n",
    "    \n",
    "# model = SentenceTransformer(\"distilbert-base-nli-stsb-mean-tokens\")\n",
    "\n",
    "# model.save(orig_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"train_df_final.json\")\n",
    "dev_df = pd.read_json(\"dev_df_final.json\")\n",
    "\n",
    "classes = sorted(list(set(train_df[\"occupation\"].values)))\n",
    "\n",
    "label_dict = {}\n",
    "\n",
    "for i, cat in enumerate(classes):\n",
    "    label_dict[cat] = i\n",
    "\n",
    "assert 49 == len(set(train_df[\"labels\"])) == len(label_dict.keys()) == len(set(dev_df[\"labels\"]))\n",
    "\n",
    "num_epochs = 5 # trained iteratively\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    model = SentenceTransformer(orig_model_path)\n",
    "    \n",
    "    train_examples = shuffle(return_examples(train_df))\n",
    "    dev_examples = shuffle(return_examples(dev_df))\n",
    "    print(\"Training examples: {}.\".format(len(train_examples)))\n",
    "    print(\"Dev examples: {}\".format(len(dev_examples)))\n",
    "    \n",
    "    # Datasets\n",
    "    train_dataset = SentencesDataset(train_examples, model)\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "    dev_dataset = SentencesDataset(dev_examples, model)\n",
    "    dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=1)    \n",
    "    \n",
    "    # Main loss function + evaluator\n",
    "    softmax_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), \n",
    "                                    num_labels=49)\n",
    "    evaluator = evaluation.LabelAccuracyEvaluator(dev_dataloader, softmax_model=softmax_loss)\n",
    "\n",
    "    # Option to add contrastive loss--not used for this model\n",
    "    cl_fraction = 0.0\n",
    "    if cl_fraction > 0.0:\n",
    "        \"\"\"If contrastive loss were used, a number of positive and negative examples would be\n",
    "        created, and a training step on those examples with contrastive loss would occur before\n",
    "        training with the softmax loss for each epoch.\"\"\"\n",
    "        num_pos = int(train_df.shape[0]/25/4 * cl_fraction)\n",
    "        num_neg = int(train_df.shape[0]/(49*48)/2 * cl_fraction)\n",
    "        contrast_train_examples = return_contrast_examples(train_df, num_pos, num_neg)\n",
    "        contrast_dataset = SentencesDataset(contrast_train_examples, model)\n",
    "        contrast_dataloader = DataLoader(contrast_dataset, shuffle=True, batch_size=16)\n",
    "        contrast_loss = losses.ContrastiveLoss(model=model)\n",
    "        # This exploits sentence_bert's multi-task learning feature\n",
    "        train_objectives = [(contrast_dataloader, contrast_loss), (train_dataloader, softmax_loss)]\n",
    "    else:\n",
    "        train_objectives = [(train_dataloader, softmax_loss)]\n",
    "\n",
    "    warmup_steps = int(len(train_examples)/10)\n",
    "    model.fit(train_objectives=train_objectives, epochs=1, warmup_steps=warmup_steps, evaluator=evaluator, evaluation_steps=0, output_path=orig_model_path)\n",
    "\n",
    "    model.save(orig_model_path)\n",
    "    \n",
    "    print(\"Dev set accuracy: {}\".format(model.best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30253061224489797"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score # dev set score on final epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84effcbdd2144f7ad0d08566c7eabb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716b74c702774fd289db086bf684b1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                                                    | 1/12250 [00:00<34:06,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████| 12250/12250 [25:42<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_acc_path = os.path.join(os.getcwd(), \"final_results/train_acc/\")\n",
    "if not os.path.isdir(train_acc_path):\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(), \"final_results/\")):\n",
    "        os.mkdir(os.path.join(os.getcwd(), \"final_results/\"))\n",
    "    os.mkdir(train_acc_path)\n",
    "\n",
    "train_acc_evaluator = evaluation.LabelAccuracyEvaluator(train_dataloader, softmax_model=softmax_loss)\n",
    "\n",
    "single_train_pair = train_df[train_df[\"labels\"]==0].sample(2)\n",
    "texts_ = [t for t in single_train_pair[\"text\"].values]\n",
    "assert type(texts_[0]) == type(texts_[1]) == str\n",
    "single_train_pair = [InputExample(texts=[texts_[0], texts_[1]], label=0)]\n",
    "# single_train_pair = return_examples(single_train_pair)\n",
    "single_train_dataset = SentencesDataset(single_train_pair, model)\n",
    "single_train_dataloader = DataLoader(single_train_dataset, shuffle=True, batch_size=1)\n",
    "\n",
    "model.fit(train_objectives=[(single_train_dataloader, softmax_loss)], epochs=1, warmup_steps=warmup_steps, \n",
    "          evaluator=train_acc_evaluator, evaluation_steps=0, output_path=train_acc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33414285714285713"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score # train set accuracy after final epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48419, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_json(\"test_df_final.json\")\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5cf5d19eed4ac19fcc8238d630a793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c0c84e43ab4975977b9703ecf29803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                                                    | 7/24208 [00:00<05:51, 68.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████| 24208/24208 [06:14<00:00, 64.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.305229676140119\n"
     ]
    }
   ],
   "source": [
    "test_acc_path = os.path.join(os.getcwd(), \"final_results/test_acc/\")\n",
    "if not os.path.isdir(train_acc_path):\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(), \"final_results/\")):\n",
    "        os.mkdir(os.path.join(os.getcwd(), \"final_results/\"))\n",
    "    os.mkdir(tests_acc_path)\n",
    "\n",
    "single_train_pair = train_df[train_df[\"labels\"]==0].sample(2)\n",
    "texts_ = [t for t in single_train_pair[\"text\"].values]\n",
    "assert type(texts_[0]) == type(texts_[1]) == str\n",
    "single_train_pair = [InputExample(texts=[texts_[0], texts_[1]], label=0)]\n",
    "single_train_dataset = SentencesDataset(single_train_pair, model)\n",
    "single_train_dataloader = DataLoader(single_train_dataset, shuffle=True, batch_size=1)\n",
    "\n",
    "\n",
    "def return_paired_examples(df_):\n",
    "    examples = []\n",
    "    for lab in label_dict.values():\n",
    "        lab_df = df_[df_[\"labels\"]==lab]\n",
    "        lab_df = shuffle(lab_df)\n",
    "        revs = [rev.lower() for rev in lab_df[\"text\"].values]\n",
    "        pairs = output_pairs(lab, revs)\n",
    "        for pair in pairs:\n",
    "            assert type(pair) == list\n",
    "            assert len(pair) == 2\n",
    "            assert type(pair[0]) == type(pair[1]) == str\n",
    "            ex = InputExample(texts=pair, label=lab)\n",
    "            examples.append(ex)\n",
    "    return examples\n",
    "\n",
    "\n",
    "test_examples = return_paired_examples(test_df)\n",
    "test_dataset = SentencesDataset(test_examples, model)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=1)\n",
    "\n",
    "test_acc_evaluator = evaluation.LabelAccuracyEvaluator(test_dataloader, softmax_model=softmax_loss)\n",
    "\n",
    "model.fit(train_objectives=[(single_train_dataloader, softmax_loss)], epochs=1, warmup_steps=warmup_steps, \n",
    "          evaluator=test_acc_evaluator, evaluation_steps=0, output_path=test_acc_path)\n",
    "\n",
    "print(\"Test accuracy: {}\".format(model.best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.305229676140119"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score # test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing and saving classification report info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"y_true_epoch0.p\", \"rb\") as reader:\n",
    "    y_true = pickle.load(reader)\n",
    "\n",
    "with open(\"y_pred_epoch0.p\", \"rb\") as reader:\n",
    "    y_pred = pickle.load(reader)\n",
    "\n",
    "report = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
    "\n",
    "report_df = pd.DataFrame(report).T.reset_index()\n",
    "\n",
    "backward_d = {str(value):key for key, value in label_dict.items()}\n",
    "\n",
    "# backward_d\n",
    "\n",
    "report_df[\"occupation\"] = [backward_d[str(lab)] if lab in [str(value) for value in label_dict.values()] else \"\" for lab in report_df[\"index\"]]\n",
    "\n",
    "report_df = report_df[[\"index\", \"occupation\", \"support\", \"f1-score\", \"precision\", \"recall\"]]\n",
    "\n",
    "report_df.to_json(\"cs230_classification_report_tuning_softmax.json\")\n",
    "\n",
    "report_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
