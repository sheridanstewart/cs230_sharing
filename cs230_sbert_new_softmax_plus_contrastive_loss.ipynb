{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an SBERT model \"from scratch\" using distilbert-base-uncased with both a softmax loss and contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from itertools import combinations\n",
    "from sentence_transformers import evaluation, InputExample, losses, models, SentencesDataset, SentenceTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla K80 True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0), torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_pairs(lab, strings):\n",
    "    \"\"\"returns pairs from a list of strings\"\"\"\n",
    "    remainder = len(strings) % 2\n",
    "    if remainder != 0:\n",
    "        strings = strings[:-remainder]\n",
    "    num_pairs = int(len(strings)/2)\n",
    "    pairs = []\n",
    "    for i in range(num_pairs):\n",
    "        begin = i*2\n",
    "        end = begin + 2\n",
    "        pair = strings[begin:end]\n",
    "        pairs.append(pair)\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def return_examples(df_):\n",
    "    \"\"\"returns paired reviews from the same class for\n",
    "    training SBERT model\"\"\"\n",
    "    examples = []\n",
    "    for lab in label_dict.values():\n",
    "        lab_df = df_[df_[\"labels\"]==lab]\n",
    "        lab_df = shuffle(lab_df)\n",
    "        texts = list(lab_df[\"text\"].values)\n",
    "        assert len(texts) % 2 == 0\n",
    "        l = int(len(texts)/2)\n",
    "        assert l > 0\n",
    "        labs = [lab] * l\n",
    "        zipped = list(zip(texts[:l], texts[l:], labs))\n",
    "        assert len(zipped) == l \n",
    "        assert l == len(texts)/2\n",
    "        for ex in zipped:\n",
    "            assert type(ex[0]) == type(ex[1]) == str\n",
    "            pair = [ex[0], ex[1]]\n",
    "            assert type(pair) == list\n",
    "            assert type(ex[2]) == int\n",
    "            ex = InputExample(texts=pair, label=ex[2])\n",
    "            examples.append(ex)\n",
    "    return examples\n",
    "\n",
    "\n",
    "def return_contrast_examples(df_, num_pos, num_neg):\n",
    "    \"\"\"single call to get positive and negative examples for\n",
    "    contrastive loss\"\"\"\n",
    "    pos = return_positive_contrast_examples(df_, num_pos)\n",
    "    neg = return_negative_contrast_examples(df_, num_neg)\n",
    "    print(\"Contrastive loss examples total: {}\".format(len(pos + neg)))\n",
    "    return pos + neg\n",
    "\n",
    "\n",
    "def return_positive_contrast_examples(df_, n):\n",
    "    \"\"\"positive examples for contrastive loss:\n",
    "    same class, labeled 1\"\"\"\n",
    "    pos_contrast_examples = []\n",
    "    for lab in label_dict.values():\n",
    "        lab_df = df_[df_[\"labels\"]==lab]\n",
    "        lab_df = shuffle(lab_df)\n",
    "        lab_df = lab_df.sample(n)\n",
    "        revs = [rev.lower() for rev in lab_df[\"text\"].values]\n",
    "        pairs = output_pairs(lab, revs)\n",
    "        for pair in pairs:\n",
    "            assert type(pair) == list\n",
    "            assert len(pair) == 2\n",
    "            assert type(pair[0]) == type(pair[1]) == str\n",
    "            ex = InputExample(texts=pair, label=1)\n",
    "            pos_contrast_examples.append(ex)\n",
    "    return pos_contrast_examples\n",
    "    \n",
    "    \n",
    "def return_negative_contrast_examples(df_, n):\n",
    "    \"\"\"negative examples for contrastive loss:\n",
    "    pairs from all combinations of classes,\n",
    "    with label = 0 indicating they are not\n",
    "    of teh same class\"\"\"\n",
    "    neg_contrast_examples = []\n",
    "    df_dict = {}\n",
    "    for lab in label_dict.values():\n",
    "        lab_df = df_[df_[\"labels\"]==lab]\n",
    "        lab_df = shuffle(lab_df)\n",
    "        lab_df = lab_df.sample(n)\n",
    "        df_dict[lab] = lab_df\n",
    "    for k, comb in enumerate(combinations(df_dict.keys(), 2)):\n",
    "        lab_i, lab_j = comb\n",
    "        df_i = df_dict[lab_i].sample(n)\n",
    "        df_j = df_dict[lab_j].sample(n)\n",
    "        texts_i = list(df_i[\"text\"].values)\n",
    "        texts_j = list(df_j[\"text\"].values)\n",
    "        neg_labs = [0] * len(texts_i)\n",
    "        assert len(neg_labs) == len(texts_i) == len(texts_j)\n",
    "        exes = list(zip(texts_i, texts_j, neg_labs))\n",
    "        for ex in exes:\n",
    "            assert type(ex[0]) == type(ex[1]) == str\n",
    "            assert ex[2] == 0\n",
    "            ex = InputExample(texts=[ex[0].lower(), ex[1].lower()], label=ex[2])\n",
    "            neg_contrast_examples.append(ex)\n",
    "    return neg_contrast_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and specify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392000, 5)\n",
      "(98000, 5)\n",
      "Num. occupations: 49. Training set shape: (392000, 5). Dev set shape: (98000, 5).\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_json(\"train_df_final.json\")\n",
    "dev_df = pd.read_json(\"dev_df_final.json\")\n",
    "\n",
    "print(train_df.shape)\n",
    "print(dev_df.shape)\n",
    "\n",
    "classes = sorted(list(set(train_df[\"occupation\"].values)))\n",
    "label_dict = {}\n",
    "\n",
    "for i, cat in enumerate(classes):\n",
    "    label_dict[cat] = i\n",
    "    \n",
    "print(\"Num. occupations: {}. Training set shape: {}. Dev set shape: {}.\".format(len(label_dict.keys()), \n",
    "                                                                                train_df.shape, dev_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL ARCHITECTURE -- commented out to avoid overwriting model.\n",
    "orig_model_path = os.path.join(os.getcwd(), \"final_proj_model/\")\n",
    "# if not os.path.isdir(orig_model_path):\n",
    "#     os.mkdir(orig_model_path)\n",
    "\n",
    "# word_embs = models.Transformer(\"distilbert-base-uncased\")\n",
    "# mean_pool = models.Pooling(word_embs.get_word_embedding_dimension(), pooling_mode_mean_tokens=True)\n",
    "# dense_relu_sm = models.Dense(in_features=mean_pool.get_sentence_embedding_dimension(), \n",
    "#                            out_features=256, activation_function=nn.ReLU())\n",
    "\n",
    "# model = SentenceTransformer(modules=[word_embs, mean_pool, dense_relu_sm])\n",
    "\n",
    "# model.save(orig_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"train_df_final.json\")\n",
    "dev_df = pd.read_json(\"dev_df_final.json\")\n",
    "\n",
    "classes = sorted(list(set(train_df[\"occupation\"].values)))\n",
    "\n",
    "label_dict = {}\n",
    "\n",
    "for i, cat in enumerate(classes):\n",
    "    label_dict[cat] = i\n",
    "\n",
    "assert 49 == len(set(train_df[\"labels\"])) == len(label_dict.keys()) == len(set(dev_df[\"labels\"]))\n",
    "\n",
    "num_epochs = 5 # trained iteratively\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    model = SentenceTransformer(orig_model_path)\n",
    "    \n",
    "    train_examples = shuffle(return_examples(train_df))\n",
    "    dev_examples = shuffle(return_examples(dev_df))\n",
    "    print(\"Training examples: {}.\".format(len(train_examples)))\n",
    "    print(\"Dev examples: {}\".format(len(dev_examples)))\n",
    "    \n",
    "    # Datasets\n",
    "    train_dataset = SentencesDataset(train_examples, model)\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "    dev_dataset = SentencesDataset(dev_examples, model)\n",
    "    dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=1)    \n",
    "    \n",
    "    # Main loss function + evaluator\n",
    "    softmax_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), \n",
    "                                    num_labels=49)\n",
    "    evaluator = evaluation.LabelAccuracyEvaluator(dev_dataloader, softmax_model=softmax_loss)\n",
    "\n",
    "    # Option to add contrastive loss--not used for this model\n",
    "    cl_fraction = 0.5\n",
    "    if cl_fraction > 0.0:\n",
    "        \"\"\"If contrastive loss were used, a number of positive and negative examples would be\n",
    "        created, and a training step on those examples with contrastive loss would occur before\n",
    "        training with the softmax loss for each epoch.\"\"\"\n",
    "        num_pos = int(train_df.shape[0]/25/4 * cl_fraction)\n",
    "        num_neg = int(train_df.shape[0]/(49*48)/2 * cl_fraction)\n",
    "        contrast_train_examples = return_contrast_examples(train_df, num_pos, num_neg)\n",
    "        contrast_dataset = SentencesDataset(contrast_train_examples, model)\n",
    "        contrast_dataloader = DataLoader(contrast_dataset, shuffle=True, batch_size=16)\n",
    "        contrast_loss = losses.ContrastiveLoss(model=model)\n",
    "        # This exploits sentence_bert's multi-task learning feature\n",
    "        train_objectives = [(contrast_dataloader, contrast_loss), (train_dataloader, softmax_loss)]\n",
    "    else:\n",
    "        train_objectives = [(train_dataloader, softmax_loss)]\n",
    "\n",
    "    warmup_steps = int(len(train_examples)/10)\n",
    "    model.fit(train_objectives=train_objectives, epochs=1, warmup_steps=warmup_steps, evaluator=evaluator, evaluation_steps=0, output_path=orig_model_path)\n",
    "\n",
    "    model.save(orig_model_path)\n",
    "    \n",
    "    print(\"Dev set accuracy: {}\".format(model.best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat final_proj_model/accuracy_evaluation_results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4527fde51b0546b1a6b719ac7db5084e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa588b8d5b94b2da821b68f4d800e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating:   0%|          | 0/12250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 12250/12250 [53:38<00:00,  3.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19539285714285715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_acc_path = os.path.join(os.getcwd(), \"final_results/train_acc/\")\n",
    "if not os.path.isdir(train_acc_path):\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(), \"final_results/\")):\n",
    "        os.mkdir(os.path.join(os.getcwd(), \"final_results/\"))\n",
    "    os.mkdir(train_acc_path)\n",
    "\n",
    "train_acc_evaluator = evaluation.LabelAccuracyEvaluator(train_dataloader, softmax_model=softmax_loss)\n",
    "\n",
    "single_train_pair = train_df[train_df[\"labels\"]==0].sample(2)\n",
    "texts_ = [t for t in single_train_pair[\"text\"].values]\n",
    "assert type(texts_[0]) == type(texts_[1]) == str\n",
    "single_train_pair = [InputExample(texts=[texts_[0], texts_[1]], label=0)]\n",
    "# single_train_pair = return_examples(single_train_pair)\n",
    "single_train_dataset = SentencesDataset(single_train_pair, model)\n",
    "single_train_dataloader = DataLoader(single_train_dataset, shuffle=True, batch_size=1)\n",
    "\n",
    "model.fit(train_objectives=[(single_train_dataloader, softmax_loss)], epochs=1, warmup_steps=warmup_steps, \n",
    "          evaluator=train_acc_evaluator, evaluation_steps=0, output_path=train_acc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_json(\"test_df_final.json\")\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6820eed565d04557937d72ec3657af4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a80949eb9c4786806d04148502e660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=1.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 3/24208 [00:00<15:58, 25.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 24208/24208 [09:37<00:00, 41.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18721083939193656\n",
      "\n",
      "Test accuracy: 0.18721083939193656\n"
     ]
    }
   ],
   "source": [
    "test_acc_path = os.path.join(os.getcwd(), \"final_results/test_acc/\")\n",
    "if not os.path.isdir(train_acc_path):\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(), \"final_results/\")):\n",
    "        os.mkdir(os.path.join(os.getcwd(), \"final_results/\"))\n",
    "    os.mkdir(tests_acc_path)\n",
    "\n",
    "single_train_pair = train_df[train_df[\"labels\"]==0].sample(2)\n",
    "texts_ = [t for t in single_train_pair[\"text\"].values]\n",
    "assert type(texts_[0]) == type(texts_[1]) == str\n",
    "single_train_pair = [InputExample(texts=[texts_[0], texts_[1]], label=0)]\n",
    "single_train_dataset = SentencesDataset(single_train_pair, model)\n",
    "single_train_dataloader = DataLoader(single_train_dataset, shuffle=True, batch_size=1)\n",
    "\n",
    "\n",
    "def return_paired_examples(df_):\n",
    "    examples = []\n",
    "    for lab in label_dict.values():\n",
    "        lab_df = df_[df_[\"labels\"]==lab]\n",
    "        lab_df = shuffle(lab_df)\n",
    "        revs = [rev.lower() for rev in lab_df[\"text\"].values]\n",
    "        pairs = output_pairs(lab, revs)\n",
    "        for pair in pairs:\n",
    "            assert type(pair) == list\n",
    "            assert len(pair) == 2\n",
    "            assert type(pair[0]) == type(pair[1]) == str\n",
    "            ex = InputExample(texts=pair, label=lab)\n",
    "            examples.append(ex)\n",
    "    return examples\n",
    "\n",
    "\n",
    "test_examples = return_paired_examples(test_df)\n",
    "test_dataset = SentencesDataset(test_examples, model)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=1)\n",
    "\n",
    "test_acc_evaluator = evaluation.LabelAccuracyEvaluator(test_dataloader, softmax_model=softmax_loss)\n",
    "\n",
    "model.fit(train_objectives=[(single_train_dataloader, softmax_loss)], epochs=1, warmup_steps=warmup_steps, \n",
    "          evaluator=test_acc_evaluator, evaluation_steps=0, output_path=test_acc_path)\n",
    "\n",
    "print(\"Test accuracy: {}\".format(model.best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18721083939193656"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score # test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing and saving classification report info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"y_true_epoch0.p\", \"rb\") as reader:\n",
    "    y_true = pickle.load(reader)\n",
    "\n",
    "with open(\"y_pred_epoch0.p\", \"rb\") as reader:\n",
    "    y_pred = pickle.load(reader)\n",
    "\n",
    "report = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
    "\n",
    "report_df = pd.DataFrame(report).T.reset_index()\n",
    "\n",
    "backward_d = {str(value):key for key, value in label_dict.items()}\n",
    "\n",
    "# backward_d\n",
    "\n",
    "report_df[\"occupation\"] = [backward_d[str(lab)] if lab in [str(value) for value in label_dict.values()] else \"\" for lab in report_df[\"index\"]]\n",
    "\n",
    "report_df = report_df[[\"index\", \"occupation\", \"support\", \"f1-score\", \"precision\", \"recall\"]]\n",
    "\n",
    "report_df.to_json(\"cs230_classification_report_scratch_softmax_contrastive.json\")\n",
    "\n",
    "report_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
