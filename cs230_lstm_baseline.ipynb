{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight from https://realpython.com/python-keras-text-classification/\n",
    "def plot_history(history):\n",
    "    \"\"\"\"\"\"\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and manipulating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"train_df_final_LSTM.json\").sample(frac=1) # sample(frac=1) is another way to shuffle data\n",
    "dev_df = pd.read_json(\"dev_df_final_LSTM.json\").sample(frac=1)\n",
    "test_df = pd.read_json(\"test_df_final_LSTM.json\").sample(frac=1)\n",
    "\n",
    "# The full corpus, mean word count per review is ~56 words, while median is 45.\n",
    "# LSTM-based models I experimented with a subset of training data really struggled,\n",
    "# and one I thing I tried was cutting the max length to mitigate the extent\n",
    "# of padding:\n",
    "\n",
    "maxlen = 50\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "\n",
    "train_text = list(train_df[\"text\"].values)\n",
    "tokenizer.fit_on_texts(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LSTM baseline was built with keras, rather than pytorch\n",
    "X_train = tokenizer.texts_to_sequences(train_text)\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\n",
    "train_y = list(train_df[\"labels\"].values)\n",
    "train_y = keras.utils.to_categorical(train_y, num_classes=49)\n",
    "assert len(train_text) == len(train_y) == train_df.shape[0] == len(X_train)\n",
    "\n",
    "dev_text = list(dev_df[\"text\"].values)\n",
    "X_dev = tokenizer.texts_to_sequences(dev_text)\n",
    "X_dev = keras.preprocessing.sequence.pad_sequences(X_dev, padding=\"post\", maxlen=maxlen)\n",
    "dev_y = dev_df[\"labels\"].values\n",
    "dev_y = keras.utils.to_categorical(dev_y, num_classes=49)\n",
    "assert len(dev_text) == len(dev_y) == dev_df.shape[0] == len(X_dev)\n",
    "\n",
    "test_text = list(test_df[\"text\"].values)\n",
    "X_test = tokenizer.texts_to_sequences(test_text)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, padding=\"post\", maxlen=maxlen)\n",
    "test_y = test_df[\"labels\"].values\n",
    "test_y = keras.utils.to_categorical(test_y, num_classes=49)\n",
    "assert len(test_text) == len(test_y) == test_df.shape[0] == len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used bits of this as a guide when switching to keras:\n",
    "# https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "# This code also drew on old mittens code of mine, which undoubtedly drew on others' \n",
    "# work (e.g., blog posts) in addition to the official mittens documentation at \n",
    "# https://github.com/roamanalytics/mittens.\n",
    "# Regrettably, I do not know where other sources of inspiration (or code) may have come from.\n",
    "\n",
    "# In early experiments with LSTM-based models on a sample of training data,\n",
    "# I compared mittens to vanilla GloVe using lighter-weight sets of pretrained \n",
    "# GloVe vectors such as the  Wikipedia/Gigaword vectors with a 400k token vocabulary.\n",
    "# Using mittens didn't make much of a difference.\n",
    "\n",
    "# When I switched to the Common Crawl GloVe vectors (vocab of 1.9 million), the computational\n",
    "# difficulty of training the mittens vectors didn't seem to offset any (unexpected)\n",
    "# increase in performance.\n",
    "\n",
    "glove_dict = pickle.load(open(\"glove_lower_lg.d\", \"rb\"))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = list(glove_dict.values())[0].shape[0]\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, emb in glove_dict.items():\n",
    "    if word in tokenizer.word_index:\n",
    "        idx = tokenizer.word_index[word]\n",
    "        embedding_matrix[idx] = np.array(emb, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I experimented with padding the zeroes in the embedding matrix and found it made no difference.\n",
    "# I still kept this small shift when training my final model.\n",
    "embedding_matrix += 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e-04,  1.00000000e-04,  1.00000000e-04, ...,\n",
       "         1.00000000e-04,  1.00000000e-04,  1.00000000e-04],\n",
       "       [-5.63020009e-03,  4.49410005e-01,  1.43660007e-01, ...,\n",
       "        -1.66549997e-01, -3.73720007e-01, -1.46770002e-01],\n",
       "       [-1.32709997e-01,  7.97179995e-02,  1.65580003e-01, ...,\n",
       "         3.04609997e-01, -2.45989995e-01,  2.15190007e-01],\n",
       "       ...,\n",
       "       [-1.29880000e-02,  3.34150000e-01,  3.22749986e-01, ...,\n",
       "        -5.35869988e-02,  2.14340000e-01, -2.10999997e-01],\n",
       "       [ 1.00000000e-04,  1.00000000e-04,  1.00000000e-04, ...,\n",
       "         1.00000000e-04,  1.00000000e-04,  1.00000000e-04],\n",
       "       [ 1.00000000e-04,  1.00000000e-04,  1.00000000e-04, ...,\n",
       "         1.00000000e-04,  1.00000000e-04,  1.00000000e-04]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392000, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting learning rate and optimizer after experimenting with layers and other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_fraction = 0.2\n",
    "# train_sample_size = int(sample_fraction * len(X_train))\n",
    "# X_train_sample = X_train[:train_sample_size]\n",
    "# train_y_sample = train_y[:train_sample_size]\n",
    "# dev_sample_size = int(sample_fraction * len(X_dev))\n",
    "# X_dev_sample = X_dev[:dev_sample_size]\n",
    "# dev_y_sample = dev_y[:dev_sample_size]\n",
    "# print(len(X_train_sample), len(train_y_sample))\n",
    "# print(len(X_dev_sample), len(dev_y_sample))\n",
    "    \n",
    "# num_epochs = 3\n",
    "# batch_size = 512\n",
    "\n",
    "# r = -4*np.random.rand(4)\n",
    "# alphas = [10**r_ for r_ in r]\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for alpha in alphas:\n",
    "#     optimizers = [keras.optimizers.Adam(learning_rate=alpha), tf.keras.optimizers.RMSprop(learning_rate=alpha),\n",
    "#                   tf.keras.optimizers.Adagrad(learning_rate=alpha), tf.keras.optimizers.SGD(learning_rate=alpha)]\n",
    "#     for optimizer in optimizers:\n",
    "        \n",
    "#         try:\n",
    "#             del model\n",
    "#         except:\n",
    "#             pass\n",
    "#         try:\n",
    "#             del history\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#         opt = optimizer\n",
    "        \n",
    "#         model = keras.models.Sequential()\n",
    "#         model.add(keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "#                                    weights=[embedding_matrix], \n",
    "#                                    input_length=50, \n",
    "#                                    trainable=False))\n",
    "#         model.add(keras.layers.Dropout(0.2))\n",
    "#         model.add(keras.layers.Bidirectional(keras.layers.LSTM(50, return_sequences=True,\n",
    "#                                                                kernel_regularizer=tf.keras.regularizers.l2(l2=1e-5),\n",
    "#                                                               activation=\"relu\")))\n",
    "#         model.add(keras.layers.Dropout(0.2))\n",
    "#         model.add(keras.layers.LSTM(50, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2=1e-5)))\n",
    "#         model.add(keras.layers.Dense(49, activation=\"softmax\"))\n",
    "\n",
    "#         model.compile(optimizer=opt, # \"adam\"\n",
    "#                       loss=\"categorical_crossentropy\",\n",
    "#                       metrics=[\"accuracy\"])\n",
    "#         model.summary()\n",
    "\n",
    "#         model.fit(X_train_sample, train_y_sample, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "#         loss, accuracy = model.evaluate(X_dev_sample, dev_y_sample)\n",
    "        \n",
    "#         result = [alpha, optimizer, loss, accuracy]\n",
    "#         results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78400 78400\n",
      "19600 19600\n",
      "0.105 <tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x0000027A0534BE80>\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 300)           23591100  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 50, 100)           140400    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               500100    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 49)                2499      \n",
      "=================================================================\n",
      "Total params: 24,299,549\n",
      "Trainable params: 708,449\n",
      "Non-trainable params: 23,591,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "154/154 [==============================] - 65s 424ms/step - loss: 3.8986 - accuracy: 0.0226\n",
      "Epoch 2/3\n",
      "154/154 [==============================] - 72s 465ms/step - loss: 3.8963 - accuracy: 0.0236\n",
      "Epoch 3/3\n",
      "154/154 [==============================] - 72s 465ms/step - loss: 3.8891 - accuracy: 0.0241\n",
      "613/613 [==============================] - 8s 13ms/step - loss: 3.8780 - accuracy: 0.0280\n",
      "\n",
      "\n",
      "\n",
      "0.105 <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x0000027A0534BF10>\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 300)           23591100  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 50, 100)           140400    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               500100    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 49)                2499      \n",
      "=================================================================\n",
      "Total params: 24,299,549\n",
      "Trainable params: 708,449\n",
      "Non-trainable params: 23,591,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "154/154 [==============================] - 79s 514ms/step - loss: 3.8995 - accuracy: 0.0210\n",
      "Epoch 2/3\n",
      "154/154 [==============================] - 78s 509ms/step - loss: 3.8983 - accuracy: 0.0229\n",
      "Epoch 3/3\n",
      "154/154 [==============================] - 78s 509ms/step - loss: 3.8972 - accuracy: 0.0227\n",
      "613/613 [==============================] - 10s 17ms/step - loss: 3.8965 - accuracy: 0.0223\n",
      "\n",
      "\n",
      "\n",
      "0.1 <tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x0000027A0534BEE0>\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 300)           23591100  \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 50, 100)           140400    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               500100    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 49)                2499      \n",
      "=================================================================\n",
      "Total params: 24,299,549\n",
      "Trainable params: 708,449\n",
      "Non-trainable params: 23,591,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "154/154 [==============================] - 82s 530ms/step - loss: 3.8991 - accuracy: 0.0217\n",
      "Epoch 2/3\n",
      "154/154 [==============================] - 81s 523ms/step - loss: 3.8965 - accuracy: 0.0243\n",
      "Epoch 3/3\n",
      "154/154 [==============================] - 80s 521ms/step - loss: 3.8876 - accuracy: 0.0259\n",
      "613/613 [==============================] - 10s 17ms/step - loss: 3.8676 - accuracy: 0.0293\n",
      "\n",
      "\n",
      "\n",
      "0.1 <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x0000027AE8DF6940>\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 50, 300)           23591100  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 50, 100)           140400    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               500100    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 49)                2499      \n",
      "=================================================================\n",
      "Total params: 24,299,549\n",
      "Trainable params: 708,449\n",
      "Non-trainable params: 23,591,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 77s 503ms/step - loss: 3.8992 - accuracy: 0.0218\n",
      "Epoch 2/3\n",
      "154/154 [==============================] - 77s 499ms/step - loss: 3.8978 - accuracy: 0.0228\n",
      "Epoch 3/3\n",
      "154/154 [==============================] - 77s 500ms/step - loss: 3.8965 - accuracy: 0.0247\n",
      "613/613 [==============================] - 10s 17ms/step - loss: 3.8959 - accuracy: 0.0240\n",
      "\n",
      "\n",
      "\n",
      "0.11 <tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x0000027AE8DF6D00>\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 50, 300)           23591100  \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 50, 100)           140400    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               500100    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 49)                2499      \n",
      "=================================================================\n",
      "Total params: 24,299,549\n",
      "Trainable params: 708,449\n",
      "Non-trainable params: 23,591,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "154/154 [==============================] - 65s 424ms/step - loss: 3.8989 - accuracy: 0.0217\n",
      "Epoch 2/3\n",
      "154/154 [==============================] - 66s 427ms/step - loss: 3.8967 - accuracy: 0.0251\n",
      "Epoch 3/3\n",
      "154/154 [==============================] - 65s 422ms/step - loss: 3.8932 - accuracy: 0.0250\n",
      "613/613 [==============================] - 8s 13ms/step - loss: 3.8909 - accuracy: 0.0271\n",
      "\n",
      "\n",
      "\n",
      "0.11 <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x0000027ACBF7AFD0>\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 50, 300)           23591100  \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 50, 100)           140400    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               500100    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 49)                2499      \n",
      "=================================================================\n",
      "Total params: 24,299,549\n",
      "Trainable params: 708,449\n",
      "Non-trainable params: 23,591,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "154/154 [==============================] - 83s 541ms/step - loss: 3.8994 - accuracy: 0.0212\n",
      "Epoch 2/3\n",
      "154/154 [==============================] - 85s 555ms/step - loss: 3.8978 - accuracy: 0.0231\n",
      "Epoch 3/3\n",
      "154/154 [==============================] - 85s 550ms/step - loss: 3.8961 - accuracy: 0.0239\n",
      "613/613 [==============================] - 12s 19ms/step - loss: 3.8951 - accuracy: 0.0234\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_fraction = 0.2\n",
    "train_sample_size = int(sample_fraction * len(X_train))\n",
    "X_train_sample = X_train[:train_sample_size]\n",
    "train_y_sample = train_y[:train_sample_size]\n",
    "dev_sample_size = int(sample_fraction * len(X_dev))\n",
    "X_dev_sample = X_dev[:dev_sample_size]\n",
    "dev_y_sample = dev_y[:dev_sample_size]\n",
    "print(len(X_train_sample), len(train_y_sample))\n",
    "print(len(X_dev_sample), len(dev_y_sample))\n",
    "    \n",
    "num_epochs = 3\n",
    "batch_size = 512\n",
    "\n",
    "# started from -4 (four digits after decimal), like in class, and tried larger samples values\n",
    "# I gradually narrowed it down, switching to -2 and trying fewer values\n",
    "# r = -2*np.random.rand(2)                 \n",
    "# alphas = [10**r_ for r_ in r]\n",
    "\n",
    "# I eventually determined that learning rates in this range were working best\n",
    "alphas = [0.105, 0.1, 0.11]\n",
    "\n",
    "results = []\n",
    "\n",
    "# I initially looped through, for each alpha, four different optimizers.\n",
    "# I eventually excluded Adam and RMSProp, as Adagrad and SGD consistently worked best.\n",
    "for alpha in alphas:\n",
    "    optimizers = [tf.keras.optimizers.Adagrad(learning_rate=alpha), tf.keras.optimizers.SGD(learning_rate=alpha)]\n",
    "    for optimizer in optimizers:\n",
    "        \n",
    "        try:\n",
    "            del model\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            del history\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        opt = optimizer\n",
    "        \n",
    "        print(alpha, optimizer)\n",
    "        \n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                                   weights=[embedding_matrix], \n",
    "                                   input_length=50, \n",
    "                                   trainable=False))\n",
    "        model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Bidirectional(keras.layers.LSTM(50, return_sequences=True,\n",
    "                                                               kernel_regularizer=tf.keras.regularizers.l2(l2=1e-5),\n",
    "                                                              activation=\"relu\")))\n",
    "        model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Bidirectional(keras.layers.LSTM(50, return_sequences=True,\n",
    "                                                               kernel_regularizer=tf.keras.regularizers.l2(l2=1e-5),\n",
    "                                                              activation=\"relu\")))\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dropout(0.2))\n",
    "        model.add(keras.layers.Dense(50, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(49, activation=\"softmax\"))\n",
    "\n",
    "        model.compile(optimizer=opt, # \"adam\"\n",
    "                      loss=\"categorical_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "        model.summary()\n",
    "\n",
    "        model.fit(X_train_sample, train_y_sample, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "        loss, accuracy = model.evaluate(X_dev_sample, dev_y_sample)\n",
    "        \n",
    "        result = [alpha, optimizer, loss, accuracy]\n",
    "        results.append(result)\n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.1,\n",
       "  <tensorflow.python.keras.optimizer_v2.adagrad.Adagrad at 0x27a0534bee0>,\n",
       "  3.867598533630371,\n",
       "  0.029285714030265808],\n",
       " [0.105,\n",
       "  <tensorflow.python.keras.optimizer_v2.adagrad.Adagrad at 0x27a0534be80>,\n",
       "  3.8780441284179688,\n",
       "  0.027959182858467102],\n",
       " [0.11,\n",
       "  <tensorflow.python.keras.optimizer_v2.adagrad.Adagrad at 0x27ae8df6d00>,\n",
       "  3.8908610343933105,\n",
       "  0.02709183655679226],\n",
       " [0.1,\n",
       "  <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x27ae8df6940>,\n",
       "  3.895852565765381,\n",
       "  0.024030612781643867],\n",
       " [0.11,\n",
       "  <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x27acbf7afd0>,\n",
       "  3.895056962966919,\n",
       "  0.0234183669090271],\n",
       " [0.105,\n",
       "  <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x27a0534bf10>,\n",
       "  3.896501064300537,\n",
       "  0.022295918315649033]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best results using a sample of the train and dev sets\n",
    "# from sampling learning rates and trying them with different optimizers\n",
    "results = sorted(results, key=lambda x: x[-1], reverse = True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final LSTM-base model for the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392000 392000\n",
      "98000 98000\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 300)           23591100  \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (None, 50, 300)           100       \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 50, 100)           140400    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               500100    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 49)                2499      \n",
      "=================================================================\n",
      "Total params: 24,299,649\n",
      "Trainable params: 708,549\n",
      "Non-trainable params: 23,591,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "766/766 [==============================] - 563s 735ms/step - loss: 3.8047 - accuracy: 0.0433 - val_loss: 3.6446 - val_accuracy: 0.0738\n",
      "Epoch 2/50\n",
      "766/766 [==============================] - 510s 666ms/step - loss: 3.6698 - accuracy: 0.0705 - val_loss: 3.5622 - val_accuracy: 0.0928\n",
      "Epoch 3/50\n",
      "766/766 [==============================] - 511s 667ms/step - loss: 3.6011 - accuracy: 0.0835 - val_loss: 3.4742 - val_accuracy: 0.1078\n",
      "Epoch 4/50\n",
      "766/766 [==============================] - 502s 655ms/step - loss: 3.5551 - accuracy: 0.0927 - val_loss: 3.4516 - val_accuracy: 0.1131\n",
      "Epoch 5/50\n",
      "766/766 [==============================] - 483s 630ms/step - loss: 3.5185 - accuracy: 0.1001 - val_loss: 3.4159 - val_accuracy: 0.1205\n",
      "Epoch 6/50\n",
      "766/766 [==============================] - 488s 637ms/step - loss: 3.4903 - accuracy: 0.1065 - val_loss: 3.3831 - val_accuracy: 0.1278\n",
      "Epoch 7/50\n",
      "766/766 [==============================] - 484s 632ms/step - loss: 3.4656 - accuracy: 0.1111 - val_loss: 3.3576 - val_accuracy: 0.1323\n",
      "Epoch 8/50\n",
      "766/766 [==============================] - 490s 640ms/step - loss: 3.4452 - accuracy: 0.1158 - val_loss: 3.3417 - val_accuracy: 0.1342\n",
      "Epoch 9/50\n",
      "766/766 [==============================] - 488s 637ms/step - loss: 3.4268 - accuracy: 0.1202 - val_loss: 3.3406 - val_accuracy: 0.1384\n",
      "Epoch 10/50\n",
      "766/766 [==============================] - 483s 631ms/step - loss: 3.4120 - accuracy: 0.1235 - val_loss: 3.3126 - val_accuracy: 0.1424\n",
      "Epoch 11/50\n",
      "766/766 [==============================] - 485s 633ms/step - loss: 3.3975 - accuracy: 0.1254 - val_loss: 3.3101 - val_accuracy: 0.1439\n",
      "Epoch 12/50\n",
      "766/766 [==============================] - 486s 635ms/step - loss: 3.3846 - accuracy: 0.1291 - val_loss: 3.3066 - val_accuracy: 0.1433\n",
      "Epoch 13/50\n",
      "766/766 [==============================] - 488s 637ms/step - loss: 3.3742 - accuracy: 0.1318 - val_loss: 3.2821 - val_accuracy: 0.1509\n",
      "Epoch 14/50\n",
      "766/766 [==============================] - 483s 631ms/step - loss: 3.3633 - accuracy: 0.1346 - val_loss: 3.2724 - val_accuracy: 0.1527\n",
      "Epoch 15/50\n",
      "766/766 [==============================] - 489s 639ms/step - loss: 3.3540 - accuracy: 0.1363 - val_loss: 3.2667 - val_accuracy: 0.1539\n",
      "Epoch 16/50\n",
      "766/766 [==============================] - 488s 637ms/step - loss: 3.3460 - accuracy: 0.1383 - val_loss: 3.2703 - val_accuracy: 0.1553\n",
      "Epoch 17/50\n",
      "766/766 [==============================] - 493s 643ms/step - loss: 3.3372 - accuracy: 0.1409 - val_loss: 3.2579 - val_accuracy: 0.1566\n",
      "Epoch 18/50\n",
      "766/766 [==============================] - 496s 648ms/step - loss: 3.3299 - accuracy: 0.1420 - val_loss: 3.2443 - val_accuracy: 0.1603\n",
      "Epoch 19/50\n",
      "766/766 [==============================] - 485s 633ms/step - loss: 3.3223 - accuracy: 0.1442 - val_loss: 3.2425 - val_accuracy: 0.1603\n",
      "Epoch 20/50\n",
      "766/766 [==============================] - 490s 639ms/step - loss: 3.3165 - accuracy: 0.1453 - val_loss: 3.2393 - val_accuracy: 0.1627\n",
      "Epoch 21/50\n",
      "766/766 [==============================] - 495s 646ms/step - loss: 3.3104 - accuracy: 0.1474 - val_loss: 3.2358 - val_accuracy: 0.1624\n",
      "Epoch 22/50\n",
      "766/766 [==============================] - 496s 648ms/step - loss: 3.3029 - accuracy: 0.1490 - val_loss: 3.2259 - val_accuracy: 0.1675\n",
      "Epoch 23/50\n",
      "766/766 [==============================] - 490s 640ms/step - loss: 3.2979 - accuracy: 0.1504 - val_loss: 3.2305 - val_accuracy: 0.1659\n",
      "Epoch 24/50\n",
      "766/766 [==============================] - 489s 639ms/step - loss: 3.2923 - accuracy: 0.1522 - val_loss: 3.2179 - val_accuracy: 0.1694\n",
      "Epoch 25/50\n",
      "766/766 [==============================] - 488s 637ms/step - loss: 3.2864 - accuracy: 0.1531 - val_loss: 3.2270 - val_accuracy: 0.1660\n",
      "Epoch 26/50\n",
      "766/766 [==============================] - 487s 636ms/step - loss: 3.2817 - accuracy: 0.1544 - val_loss: 3.2018 - val_accuracy: 0.1715\n",
      "Epoch 27/50\n",
      "766/766 [==============================] - 481s 628ms/step - loss: 3.2774 - accuracy: 0.1557 - val_loss: 3.1976 - val_accuracy: 0.1734\n",
      "Epoch 28/50\n",
      "766/766 [==============================] - 480s 627ms/step - loss: 3.2723 - accuracy: 0.1567 - val_loss: 3.2100 - val_accuracy: 0.1712\n",
      "Epoch 29/50\n",
      "766/766 [==============================] - 483s 630ms/step - loss: 3.2673 - accuracy: 0.1576 - val_loss: 3.1964 - val_accuracy: 0.1741\n",
      "Epoch 30/50\n",
      "766/766 [==============================] - 496s 647ms/step - loss: 3.2624 - accuracy: 0.1591 - val_loss: 3.1944 - val_accuracy: 0.1746\n",
      "Epoch 31/50\n",
      "766/766 [==============================] - 505s 659ms/step - loss: 3.2589 - accuracy: 0.1600 - val_loss: 3.2032 - val_accuracy: 0.1724\n",
      "Epoch 32/50\n",
      "766/766 [==============================] - 492s 642ms/step - loss: 3.2559 - accuracy: 0.1608 - val_loss: 3.1934 - val_accuracy: 0.1752\n",
      "Epoch 33/50\n",
      "766/766 [==============================] - 488s 637ms/step - loss: 3.2516 - accuracy: 0.1622 - val_loss: 3.1919 - val_accuracy: 0.1759\n",
      "Epoch 34/50\n",
      "766/766 [==============================] - 490s 640ms/step - loss: 3.2494 - accuracy: 0.1626 - val_loss: 3.1886 - val_accuracy: 0.1771\n",
      "Epoch 35/50\n",
      "766/766 [==============================] - 489s 638ms/step - loss: 3.2439 - accuracy: 0.1641 - val_loss: 3.1872 - val_accuracy: 0.1778\n",
      "Epoch 36/50\n",
      "766/766 [==============================] - 488s 637ms/step - loss: 3.2406 - accuracy: 0.1650 - val_loss: 3.1820 - val_accuracy: 0.1787\n",
      "Epoch 37/50\n",
      "766/766 [==============================] - 499s 651ms/step - loss: 3.2364 - accuracy: 0.1660 - val_loss: 3.1704 - val_accuracy: 0.1811\n",
      "Epoch 38/50\n",
      "766/766 [==============================] - 494s 646ms/step - loss: 3.2358 - accuracy: 0.1658 - val_loss: 3.1733 - val_accuracy: 0.1801\n",
      "Epoch 39/50\n",
      "766/766 [==============================] - 483s 631ms/step - loss: 3.2324 - accuracy: 0.1671 - val_loss: 3.1872 - val_accuracy: 0.1783\n",
      "Epoch 40/50\n",
      "766/766 [==============================] - 491s 641ms/step - loss: 3.2277 - accuracy: 0.1688 - val_loss: 3.1683 - val_accuracy: 0.1820\n",
      "Epoch 41/50\n",
      "766/766 [==============================] - 487s 636ms/step - loss: 3.2265 - accuracy: 0.1688 - val_loss: 3.1833 - val_accuracy: 0.1809\n",
      "Epoch 42/50\n",
      "766/766 [==============================] - 492s 642ms/step - loss: 3.2225 - accuracy: 0.1695 - val_loss: 3.1653 - val_accuracy: 0.1831\n",
      "Epoch 43/50\n",
      "766/766 [==============================] - 490s 640ms/step - loss: 3.2207 - accuracy: 0.1703 - val_loss: 3.1622 - val_accuracy: 0.1842\n",
      "Epoch 44/50\n",
      "766/766 [==============================] - 491s 641ms/step - loss: 3.2180 - accuracy: 0.1714 - val_loss: 3.1577 - val_accuracy: 0.1855\n",
      "Epoch 45/50\n",
      "766/766 [==============================] - 494s 645ms/step - loss: 3.2168 - accuracy: 0.1716 - val_loss: 3.1605 - val_accuracy: 0.1847\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766/766 [==============================] - 493s 643ms/step - loss: 3.2142 - accuracy: 0.1718 - val_loss: 3.1511 - val_accuracy: 0.1867\n",
      "Epoch 47/50\n",
      "766/766 [==============================] - 485s 633ms/step - loss: 3.2121 - accuracy: 0.1722 - val_loss: 3.1558 - val_accuracy: 0.1853\n",
      "Epoch 48/50\n",
      "766/766 [==============================] - 487s 636ms/step - loss: 3.2092 - accuracy: 0.1732 - val_loss: 3.1461 - val_accuracy: 0.1880\n",
      "Epoch 49/50\n",
      "766/766 [==============================] - 484s 632ms/step - loss: 3.2069 - accuracy: 0.1744 - val_loss: 3.1552 - val_accuracy: 0.1846\n",
      "Epoch 50/50\n",
      "766/766 [==============================] - 479s 625ms/step - loss: 3.2051 - accuracy: 0.1737 - val_loss: 3.1442 - val_accuracy: 0.1896\n",
      "12250/12250 [==============================] - 164s 13ms/step - loss: 3.0799 - accuracy: 0.2012\n",
      "Train set accuracy: 0.2012\n",
      "3063/3063 [==============================] - 40s 13ms/step - loss: 3.1442 - accuracy: 0.1896\n",
      "Dev set accuracy:  0.1896\n",
      "WARNING:tensorflow:From C:\\Users\\case\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\case\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: LSTM_results/assets\n",
      "1514/1514 [==============================] - 36s 24ms/step - loss: 3.1370 - accuracy: 0.1872\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del history\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sample_fraction = 1.0\n",
    "\n",
    "if sample_fraction < 1.0:\n",
    "    train_sample_size = int(sample_fraction * len(X_train))\n",
    "    X_train_sample = X_train[:train_sample_size]\n",
    "    train_y_sample = train_y[:train_sample_size]\n",
    "    dev_sample_size = int(sample_fraction * len(X_dev))\n",
    "    X_dev_sample = X_dev[:dev_sample_size]\n",
    "    dev_y_sample = dev_y[:dev_sample_size]\n",
    "else:\n",
    "    X_train_sample = X_train\n",
    "    train_y_sample = train_y\n",
    "    X_dev_sample = X_dev\n",
    "    dev_y_sample = dev_y\n",
    "\n",
    "print(len(X_train_sample), len(train_y_sample))\n",
    "print(len(X_dev_sample), len(dev_y_sample))\n",
    "    \n",
    "num_epochs = 50\n",
    "batch_size = 512\n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=0.1) # I had determined that Adagrad with alpha=0.1 worked best\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=50, \n",
    "                           trainable=False))\n",
    "model.add(keras.layers.LayerNormalization(axis=1))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(50, return_sequences=True,\n",
    "                                                       dropout=0.5,\n",
    "                                                      activation=\"tanh\")))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(50, return_sequences=True,\n",
    "                                                       dropout=0.5,\n",
    "                                                      activation=\"tanh\")))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(100, activation=\"tanh\"))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(50, activation=\"tanh\"))\n",
    "model.add(keras.layers.Dense(49, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_sample, train_y_sample, epochs=num_epochs, verbose=True,\n",
    "                    validation_data=(X_dev_sample, dev_y_sample), batch_size=batch_size,\n",
    "                   shuffle = True)\n",
    "loss, accuracy = model.evaluate(X_train_sample, train_y_sample, verbose=True)\n",
    "print(\"Train set accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_dev_sample, dev_y_sample, verbose=True)\n",
    "print(\"Dev set accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "try:\n",
    "    model.save(\"LSTM_results/\")\n",
    "except:\n",
    "    print(\"failed to save\")\n",
    "\n",
    "try:\n",
    "    loss, accuracy = model.evaluate(X_test, test_y, verbose=True)\n",
    "    with open(\"lstm_test_results.txt\", \"w\") as writer:\n",
    "        writer.write(\"{}\\n{}\".format(loss, accuracy))\n",
    "except:\n",
    "    print(\"failed to write test loss and accuracy\")\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(history.history, open(\"lstm_baseline_local_history_again.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1514/1514 [==============================] - 35s 23ms/step - loss: 3.1370 - accuracy: 0.1872\n",
      "3.137021780014038\n",
      "Test accuracy:  0.18719923496246338\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, test_y, verbose=True)\n",
    "\n",
    "print(loss)\n",
    "print(\"Test accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18719923996778123\n"
     ]
    }
   ],
   "source": [
    "# double-checking accuracy by hand\n",
    "assert y_pred.shape[0] == test_y.shape[0]\n",
    "assert y_pred.shape[1] == test_y.shape[1]\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i, yhat in enumerate(y_pred):\n",
    "    pred = np.argmax(yhat)\n",
    "    actual = np.argmax(test_y[i])\n",
    "    if pred == actual:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing and saving classification report info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>occupation</th>\n",
       "      <th>support</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>academic_counselor</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.242841</td>\n",
       "      <td>0.284182</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>account_executive</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.221024</td>\n",
       "      <td>0.200653</td>\n",
       "      <td>0.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>accounting</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.108573</td>\n",
       "      <td>0.074237</td>\n",
       "      <td>0.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>administrative</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.070524</td>\n",
       "      <td>0.064356</td>\n",
       "      <td>0.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>analyst</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.007401</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>beauty</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.399441</td>\n",
       "      <td>0.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>branch_manager</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>business_analyst</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.087758</td>\n",
       "      <td>0.064336</td>\n",
       "      <td>0.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>claims</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.193564</td>\n",
       "      <td>0.127370</td>\n",
       "      <td>0.403000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>client_services</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>0.007540</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.004061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>corporate_account_manager</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.025619</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>customer_service</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.098937</td>\n",
       "      <td>0.092003</td>\n",
       "      <td>0.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>designer</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.238372</td>\n",
       "      <td>0.436170</td>\n",
       "      <td>0.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>driver</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.407609</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>editor</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.301609</td>\n",
       "      <td>0.457317</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>engineer</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.118490</td>\n",
       "      <td>0.169776</td>\n",
       "      <td>0.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>field_sales_manager</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.101179</td>\n",
       "      <td>0.099421</td>\n",
       "      <td>0.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>finance_specialist</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.139907</td>\n",
       "      <td>0.209581</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>food_services</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.151229</td>\n",
       "      <td>0.204429</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>front_desk</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.138107</td>\n",
       "      <td>0.141361</td>\n",
       "      <td>0.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>hr_specialist</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.059673</td>\n",
       "      <td>0.057514</td>\n",
       "      <td>0.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>it</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.154599</td>\n",
       "      <td>0.151341</td>\n",
       "      <td>0.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>management_consulting</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.233569</td>\n",
       "      <td>0.236196</td>\n",
       "      <td>0.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>marketing_manager</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.103033</td>\n",
       "      <td>0.088131</td>\n",
       "      <td>0.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>medical_technician</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.132151</td>\n",
       "      <td>0.340249</td>\n",
       "      <td>0.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>merchandiser</td>\n",
       "      <td>812.000000</td>\n",
       "      <td>0.160684</td>\n",
       "      <td>0.262570</td>\n",
       "      <td>0.115764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>nursing</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.401681</td>\n",
       "      <td>0.376862</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>operations</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.012927</td>\n",
       "      <td>0.084337</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>patient_care_technician</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.259272</td>\n",
       "      <td>0.196493</td>\n",
       "      <td>0.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>police_&amp;_security_officers</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.277886</td>\n",
       "      <td>0.228296</td>\n",
       "      <td>0.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>program_coordinator</td>\n",
       "      <td>817.000000</td>\n",
       "      <td>0.058236</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.042840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>project_manager</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.032730</td>\n",
       "      <td>0.118012</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>quality_assurance</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.033167</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>recruiter</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.192005</td>\n",
       "      <td>0.133621</td>\n",
       "      <td>0.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>researcher</td>\n",
       "      <td>929.000000</td>\n",
       "      <td>0.301749</td>\n",
       "      <td>0.467269</td>\n",
       "      <td>0.222820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>retail_representative</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.115766</td>\n",
       "      <td>0.128993</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>sales_representative</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.094767</td>\n",
       "      <td>0.161836</td>\n",
       "      <td>0.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>server</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.392258</td>\n",
       "      <td>0.289524</td>\n",
       "      <td>0.608000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>skilled_labor</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.158872</td>\n",
       "      <td>0.121069</td>\n",
       "      <td>0.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>software_engineer</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.217771</td>\n",
       "      <td>0.223865</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>stock_clerk</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.214152</td>\n",
       "      <td>0.155880</td>\n",
       "      <td>0.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>store_manager</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.282750</td>\n",
       "      <td>0.249048</td>\n",
       "      <td>0.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>systems_administrator</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.042940</td>\n",
       "      <td>0.123223</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>systems_technician</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.034934</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>teacher</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.569498</td>\n",
       "      <td>0.550373</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.115412</td>\n",
       "      <td>0.194774</td>\n",
       "      <td>0.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>teller</td>\n",
       "      <td>876.000000</td>\n",
       "      <td>0.385356</td>\n",
       "      <td>0.290867</td>\n",
       "      <td>0.570776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>underwriter</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.196319</td>\n",
       "      <td>0.200837</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>unskilled_labor</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.143623</td>\n",
       "      <td>0.111729</td>\n",
       "      <td>0.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>accuracy</td>\n",
       "      <td></td>\n",
       "      <td>0.187199</td>\n",
       "      <td>0.187199</td>\n",
       "      <td>0.187199</td>\n",
       "      <td>0.187199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>macro avg</td>\n",
       "      <td></td>\n",
       "      <td>48419.000000</td>\n",
       "      <td>0.170579</td>\n",
       "      <td>0.197401</td>\n",
       "      <td>0.187352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td></td>\n",
       "      <td>48419.000000</td>\n",
       "      <td>0.170350</td>\n",
       "      <td>0.196960</td>\n",
       "      <td>0.187199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           index                  occupation       support  f1-score  \\\n",
       "0              0          academic_counselor   1000.000000  0.242841   \n",
       "1              1           account_executive   1000.000000  0.221024   \n",
       "2              2                  accounting   1000.000000  0.108573   \n",
       "3              3              administrative   1000.000000  0.070524   \n",
       "4              4                     analyst   1000.000000  0.007401   \n",
       "5              5                      beauty   1000.000000  0.333333   \n",
       "6              6              branch_manager   1000.000000  0.003956   \n",
       "7              7            business_analyst   1000.000000  0.087758   \n",
       "8              8                      claims   1000.000000  0.193564   \n",
       "9              9             client_services    985.000000  0.007540   \n",
       "10            10   corporate_account_manager   1000.000000  0.025619   \n",
       "11            11            customer_service   1000.000000  0.098937   \n",
       "12            12                    designer   1000.000000  0.238372   \n",
       "13            13                      driver   1000.000000  0.390625   \n",
       "14            14                      editor   1000.000000  0.301609   \n",
       "15            15                    engineer   1000.000000  0.118490   \n",
       "16            16         field_sales_manager   1000.000000  0.101179   \n",
       "17            17          finance_specialist   1000.000000  0.139907   \n",
       "18            18               food_services   1000.000000  0.151229   \n",
       "19            19                  front_desk   1000.000000  0.138107   \n",
       "20            20               hr_specialist   1000.000000  0.059673   \n",
       "21            21                          it   1000.000000  0.154599   \n",
       "22            22       management_consulting   1000.000000  0.233569   \n",
       "23            23           marketing_manager   1000.000000  0.103033   \n",
       "24            24          medical_technician   1000.000000  0.132151   \n",
       "25            25                merchandiser    812.000000  0.160684   \n",
       "26            26                     nursing   1000.000000  0.401681   \n",
       "27            27                  operations   1000.000000  0.012927   \n",
       "28            28     patient_care_technician   1000.000000  0.259272   \n",
       "29            29  police_&_security_officers   1000.000000  0.277886   \n",
       "30            30         program_coordinator    817.000000  0.058236   \n",
       "31            31             project_manager   1000.000000  0.032730   \n",
       "32            32           quality_assurance   1000.000000  0.033167   \n",
       "33            33                   recruiter   1000.000000  0.192005   \n",
       "34            34                  researcher    929.000000  0.301749   \n",
       "35            35       retail_representative   1000.000000  0.115766   \n",
       "36            36        sales_representative   1000.000000  0.094767   \n",
       "37            37                      server   1000.000000  0.392258   \n",
       "38            38               skilled_labor   1000.000000  0.158872   \n",
       "39            39           software_engineer   1000.000000  0.217771   \n",
       "40            40                 stock_clerk   1000.000000  0.214152   \n",
       "41            41               store_manager   1000.000000  0.282750   \n",
       "42            42       systems_administrator   1000.000000  0.042940   \n",
       "43            43          systems_technician   1000.000000  0.034934   \n",
       "44            44                     teacher   1000.000000  0.569498   \n",
       "45            45           technical_support   1000.000000  0.115412   \n",
       "46            46                      teller    876.000000  0.385356   \n",
       "47            47                 underwriter   1000.000000  0.196319   \n",
       "48            48             unskilled_labor   1000.000000  0.143623   \n",
       "49      accuracy                                  0.187199  0.187199   \n",
       "50     macro avg                              48419.000000  0.170579   \n",
       "51  weighted avg                              48419.000000  0.170350   \n",
       "\n",
       "    precision    recall  \n",
       "0    0.284182  0.212000  \n",
       "1    0.200653  0.246000  \n",
       "2    0.074237  0.202000  \n",
       "3    0.064356  0.078000  \n",
       "4    0.049383  0.004000  \n",
       "5    0.399441  0.286000  \n",
       "6    0.181818  0.002000  \n",
       "7    0.064336  0.138000  \n",
       "8    0.127370  0.403000  \n",
       "9    0.052632  0.004061  \n",
       "10   0.087719  0.015000  \n",
       "11   0.092003  0.107000  \n",
       "12   0.436170  0.164000  \n",
       "13   0.407609  0.375000  \n",
       "14   0.457317  0.225000  \n",
       "15   0.169776  0.091000  \n",
       "16   0.099421  0.103000  \n",
       "17   0.209581  0.105000  \n",
       "18   0.204429  0.120000  \n",
       "19   0.141361  0.135000  \n",
       "20   0.057514  0.062000  \n",
       "21   0.151341  0.158000  \n",
       "22   0.236196  0.231000  \n",
       "23   0.088131  0.124000  \n",
       "24   0.340249  0.082000  \n",
       "25   0.262570  0.115764  \n",
       "26   0.376862  0.430000  \n",
       "27   0.084337  0.007000  \n",
       "28   0.196493  0.381000  \n",
       "29   0.228296  0.355000  \n",
       "30   0.090909  0.042840  \n",
       "31   0.118012  0.019000  \n",
       "32   0.097087  0.020000  \n",
       "33   0.133621  0.341000  \n",
       "34   0.467269  0.222820  \n",
       "35   0.128993  0.105000  \n",
       "36   0.161836  0.067000  \n",
       "37   0.289524  0.608000  \n",
       "38   0.121069  0.231000  \n",
       "39   0.223865  0.212000  \n",
       "40   0.155880  0.342000  \n",
       "41   0.249048  0.327000  \n",
       "42   0.123223  0.026000  \n",
       "43   0.137931  0.020000  \n",
       "44   0.550373  0.590000  \n",
       "45   0.194774  0.082000  \n",
       "46   0.290867  0.570776  \n",
       "47   0.200837  0.192000  \n",
       "48   0.111729  0.201000  \n",
       "49   0.187199  0.187199  \n",
       "50   0.197401  0.187352  \n",
       "51   0.196960  0.187199  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ = [np.argmax(yhat) for yhat in y_pred]\n",
    "y_true_ = [np.argmax(y_) for y_ in test_y]\n",
    "\n",
    "report = classification_report(y_true_, y_pred_, zero_division=0, output_dict=True)\n",
    "\n",
    "report_df = pd.DataFrame(report).T.reset_index()\n",
    "\n",
    "backward_d = {str(value):key for key, value in label_dict.items()}\n",
    "\n",
    "report_df[\"occupation\"] = [backward_d[str(lab)] if lab in [str(value) for value in label_dict.values()] else \"\" for lab in report_df[\"index\"]]\n",
    "\n",
    "report_df = report_df[[\"index\", \"occupation\", \"support\", \"f1-score\", \"precision\", \"recall\"]]\n",
    "\n",
    "report_df.to_json(\"cs230_classification_report_LSTM_local.json\")\n",
    "\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occupation</th>\n",
       "      <th>support</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td></td>\n",
       "      <td>0.187199</td>\n",
       "      <td>0.187199</td>\n",
       "      <td>0.187199</td>\n",
       "      <td>0.187199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td></td>\n",
       "      <td>48419.000000</td>\n",
       "      <td>0.170579</td>\n",
       "      <td>0.197401</td>\n",
       "      <td>0.187352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td></td>\n",
       "      <td>48419.000000</td>\n",
       "      <td>0.170350</td>\n",
       "      <td>0.196960</td>\n",
       "      <td>0.187199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             occupation       support  f1-score  precision    recall\n",
       "index                                                               \n",
       "accuracy                     0.187199  0.187199   0.187199  0.187199\n",
       "macro avg                48419.000000  0.170579   0.197401  0.187352\n",
       "weighted avg             48419.000000  0.170350   0.196960  0.187199"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df.set_index(\"index\").loc[[\"accuracy\", \"macro avg\", \"weighted avg\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
